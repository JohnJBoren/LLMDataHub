# Datasets for Better LLM Training
## What is this?
A guide for the most frequently used datasets for LLM pretrain/instruction finetune/RLHF.
## Open Access Datasets

| Dataset name           | Links                                                                                                                                                                                                                                                                               | Used by                                                                     | Used for                          | Language                             | Size                    | Description                                                                                                                                                                                    |
|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|-----------------------------------|--------------------------------------|-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Common Crawl           | [Homepage](https://commoncrawl.org/)                                                                                                                                                                                                                                                | LLaMA(After some process)                                                   | building other datasets, pretrain | /                                    | /                       | The most famous raw dataset, not processed, rarely be used directly. One possible preprocess pipeline is [CCNet](https://github.com/facebookresearch/cc_net)                                   |
| The Pile (V1)          | [Homepage](https://pile.eleuther.ai/)                                                                                                                                                                                                                                               | GLM(partly), LLaMA(partly), GPT-J, GPT-NeoX-20B,Cerebras-GPT 6.7B, OPT-175b | pretrain                          | Multilingual, code                   | 825GB                   | A diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. Included many domain and tasks.                                       |
| C4                     | [Huggingface dataset](https://huggingface.co/datasets/c4) <br/> [TensorFlow dataset](https://www.tensorflow.org/datasets/catalog/c4)                                                                                                                                                | Google T5 Series, LLaMA                                                     | pretrain                          | English                              | 305GB                   | A colossal, cleaned version of Common Crawl's web crawl corpus. Frequently be used.                                                                                                            |
| ROOTS                  | [Homepage](https://huggingface.co/bigscience-data)                                                                                                                                                                                                                                  | BLOOM                                                                       | pretrain                          | Multilingual, code                   | 1.6TB                   | A diverse, open source dataset. Consists many different sub-datasets like Wikipedia in different language, stackexchange, etc.                                                                 |
| Pushshift reddit       | [Homepage](https://files.pushshift.io/reddit/) <br/> [paper](https://arxiv.org/pdf/2001.08435.pdf)                                                                                                                                                                                  | OPT-175b                                                                    | pretrain                          | /                                    | /                       | Raw reddit data, not processed, one possible processing pipeline in [this paper](https://aclanthology.org/2021.eacl-main.24.pdf)                                                               |
| Gutenberg project      | [Homepage](https://www.gutenberg.org/) <br/> [How to crawl it](https://www.gutenberg.org/policy/robot_access.html)                                                                                                                                                                  | LLaMA                                                                       | pretrain                          | Multilingual                         | /                       | A book dataset, mostly novels. Not be preprocessed.                                                                                                                                            |
| CLUECorpus             | [GitHub](https://github.com/CLUEbenchmark/CLUE)                                                                                                                                                                                                                                     | /                                                                           | pretrain, finetune, evaluation    | Chinese                              | 100GB                   | A Chinese pretraining Corpus sourced from *Common Crawl*.                                                                                                                                      |
| Natural Instruction    | [Homepage](https://instructions.apps.allenai.org/) <br/> [GitHub&Download](https://github.com/allenai/natural-instructions)                                                                                                                                                         | tk-instruct series                                                          | instruction finetune, evaluation  | Multilingual                         | /                       | A benchmark contains 1,600+ tasks with natural language definition and instruction. Aiming at evaluate and improve LLM's multi-task generalization ability under natural language instruction. |
| Alpaca data            | [Homepage](https://github.com/tatsu-lab/stanford_alpaca#data-release) <br/> [Download](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)                                                                                                                     | Alpaca, ChatGLM-finetune-LoRA                                               | dialog/instruction finetune       | English                              | 52K entries<br/>21.4MB  | A dataset aims to let LM follow human instruction better, generated by *text-davinci-003*.                                                                                                     |
| OIG                    | [Huggingface dataset](https://huggingface.co/datasets/laion/OIG) <br/> [OIG-small-chip2](https://huggingface.co/datasets/0-hero/OIG-small-chip2)                                                                                                                                    | Pythia-Chat-Base-7B, GPT-NeoXT-Chat-Base-20B                                | dialog/instruction finetune       | English, code                        | 44M entries             | A large, multi-task, multi-round conversational instruction dataset. Its owner claim that it is of medium quality along with a smaller high quality instruction dataset *(OIG-small-chip2)*    |
| ChatAlpaca data        | [Homepage](https://github.com/cascip/ChatAlpaca)                                                                                                                                                                                                                                    | /                                                                           | dialog/instruction finetune       | English, Chinese version coming soon | 10k entries<br/>39.5MB  | A dataset aims to help researchers develop models for instruction-following in multi-turn conversations.                                                                                       |
| Firefly(流萤)-train-1.1M | [Huggingface dataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)                                                                                                                                                                                                  | Firefly(流萤)                                                                 | instruction finetune              | Chinese                              | 1.1M entries<br/>1.17GB | A Chinese instruction-tuning dataset, contains 1.1M examples varied in 23 tasks, with human written instructions. No conversation.                                                             |
| BELLE                  | [GitHub](https://github.com/LianjiaTech/BELLE) <br/> [0.5M version](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) <br/> [1M version](https://huggingface.co/datasets/BelleGroup/train_1M_CN) <br/> [2M version](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | BELLE series, Chunhua(春华)                                                   | instruction finetune              | Chinese                              | 2.67B in total          | A dataset similar with *Alpaca data*(i.e. write seed tasks instruction, generate answers by OpenAI *text-davinci-003*) but in Chinese. No conversation.                                        |
| GuanacoDataset         | [Huggingface dataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset#guanacodataset)                                                                                                                                                                                 | Guanaco                                                                     | dialog/instruction finetune       | English, Chinese, Japanese           | 534,530 entries         | An instruction dataset designed to enhance the multilingual capabilities and address various linguistic tasks, like  natural language understanding, explicit content recognition, etc.        |
| xP3                    | [Huggingface dataset](https://huggingface.co/datasets/bigscience/xP3)                                                                                                                                                                                                               | BLOOMZ, mT0                                                                 | instruction finetune              | Multilingual, code                   | 79M entries<br/>88GB    | An instruction dataset aiming at improve LM's generalization ability, similar with *Natural Instruct*                                                                                          |
| hh-rlhf                | [GitHub](https://github.com/anthropics/hh-rlhf) <br/> [Huggingface Dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf)                                                                                                                                                      | /                                                                           | RLHF                              | English                              | 161k pairs<br/>79.3MB   | A pairwise dataset aiming at improve LM's harmlessness and helpfulness, can be used to train reward model in RLHF.                                                                             |

### Possible Overlap

We consider row items as subject.

|         | OIG      | hh-rlhf  | xP3     |
|---------|----------|----------|---------|
| OIG     |          | contains | overlap |
| hh-rlhf | part of  |          |         |
| xP3     | overlap  |          |         |

## Private Datasets
| Dataset name          | Used by            | Used for | Language                              | Size  | Description                                                                                     |
|-----------------------|--------------------|----------|---------------------------------------|-------|-------------------------------------------------------------------------------------------------|
| WebText(Reddit links) | GPT-2              | pretrain | English                               | /     | Data crawled from Reddit and filtered for GPT-2 pretraining.                                    |
| MassiveText           | Gopher, Chinchilla | pretrain | 99% English, 1% other(including code) |       |                                                                                                 |
| WuDao(悟道) Corpora     | GLM                | pretrain | Chinese                               | 200GB | A large scale Chinese corpus, Possible component originally open-sourced but not available now. |
