# üìî LLMDataHub: Awesome Datasets for LLM Training 

## Introduction üìÑ
Large language models (LLMs), such as OpenAI's GPT series, Google's Bard, and Baidu's Wenxin Yiyan, are driving profound technological changes. Recently, with the emergence of open-source large model frameworks like LlaMa and ChatGLM, training an LLM is no longer the exclusive domain of resource-rich companies. Training LLMs by small organizations or individuals has become an important interest in the open-source community, with some notable works including Alpaca, Vicuna, and Luotuo. In addition to large model frameworks, large-scale and high-quality training corpora are also essential for training large language models. Currently, relevant open-source corpora in the community are still scattered. Therefore, the goal of this repository is to continuously collect high-quality training corpora for LLMs in the open-source community.



Training a chatbot LLM that can follow human instruction effectively requires access to high-quality datasets that cover a range of conversation domains and styles. In this repository, we provide a curated collection of datasets specifically designed for chatbot training, including links, size, language, usage, and a brief description of each dataset. Our goal is to make it easier for researchers and practitioners to identify and select the most relevant and useful datasets for their chatbot LLM training needs. Whether you're working on improving chatbot dialogue quality, response generation, or language understanding, this repository has something for you.

### Contact üì¨ <br/> 
If you want to contribute, you can contact: 

  [Junhao Zhao](zhaol9555@gmail.com) üìß <br/>
  Advised by [Prof. Wanyun Cui](https://cuiwanyun.github.io/) [![](https://img.shields.io/badge/GitHub.io-@cuiwanyun-green.svg)](https://cuiwanyun.github.io/)

## General Open Access Datasets for Alignment üü¢:
#### Type Tags üè∑Ô∏è:
- SFT: Supervised Finetune
  - Dialog: Each entry contains continuous conversations 
  - Pairs: Each entry is an input-output pair 
  - Context: Each entry has a context text and related QA pairs
- PT: pretrain
- CoT: Chain-of-Thought Finetune
- RLHF: train reward model in Reinforcement Learning with Human Feedback 

### Datasets released before June 2023
| Dataset name                                                                                                                                                                                                                                                                       | Used by                                             | Type                         | Language                                             | Size                                                                                    | Description Ô∏è                                                                                                                                                                                          |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|------------------------------|------------------------------------------------------|-----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ultraChat](https://huggingface.co/datasets/stingning/ultrachat)                                                                                                                                                                                                                   | /                                                   | Dialog                       | English                                              | 1.57M dialogs                                                                           | A large scale dialog dataset created by using two ChatGPT, one of which act as the user, another generates response.                                                                                   |
| [ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)                                                                                                                                                                            | Vicuna                                              | Pairs                        | Multilingual                                         | 53K entries                                                                             | Cleaned ShareGPT dataset.                                                                                                                                                                              |
| [pku-saferlhf-dataset](https://github.com/PKU-Alignment/safe-rlhf#pku-saferlhf-dataset)                                                                                                                                                                                            | Beaver                                              | RLHF                         | English                                              | 10K + 1M                                                                                | The first dataset of its kind and contains 10k instances with safety preferences.                                                                                                                      |
| [RefGPT-Dataset](https://github.com/ziliwangnlp/RefGPT)                                                                                                                                                                                                                            | RefGPT                                              | Pairs, Dialog                | Chinese                                              | ~50K entries                                                                            | A Chinese dialog dataset aims at improve the correctness of fact in LLMs (mitigate the hallucination of LLM).                                                                                          |
| [Luotuo-QA-A<br/>CoQA-Chinese](https://huggingface.co/datasets/silk-road/Luotuo-QA-A-CoQA-Chinese)                                                                                                                                                                                 | Luotuo project                                      | Context                      | Chinese                                              | 127K QA pairs                                                                           | A dataset built upon translated CoQA. Augmented by using OpenAI API.                                                                                                                                   |
| [Wizard-LM-Chinese<br/>instruct-evol](https://huggingface.co/datasets/silk-road/Wizard-LM-Chinese-instruct-evol)                                                                                                                                                                   | Luotuo project                                      | Pairs                        | Chinese                                              | ~70K entries                                                                            | Chinese version WizardLM 70K. Answers are obtained by feed translated questions in OpenAI's GPT API and then get responses.                                                                            |
| [alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset)                                                                                                                                                                                                     | /                                                   | Pairs                        | Chinese                                              | /                                                                                       | GPT-4 translated alpaca data includes some complement data (like Chinese poetry, application, etc.). Inspected by human.                                                                               |
| [Zhihu-KOL](https://huggingface.co/datasets/wangrui6/Zhihu-KOL)                                                                                                                                                                                                                    | Open Assistant                                      | Pairs                        | Chinese                                              | 1.5GB                                                                                   | QA data on well-know Chinese Zhihu QA platform.                                                                                                                                                        |
| [Alpaca-GPT-4_zh-cn](https://huggingface.co/datasets/shibing624/alpaca-zh)                                                                                                                                                                                                         | /                                                   | Pairs                        | Chinese                                              | about 50K entries                                                                       | A Chinese Alpaca-style dataset, generated by GPT-4 originally in Chinese, not translated.                                                                                                              |
| [hh-rlhf](https://github.com/anthropics/hh-rlhf) <br/> [on Huggingface](https://huggingface.co/datasets/Anthropic/hh-rlhf)                                                                                                                                                         | Koala                                               | RLHF                         | English                                              | 161k pairs<br/>79.3MB                                                                   | A pairwise dataset for training reward models in reinforcement learning for improving language models' harmlessness and helpfulness.                                                                   |
| [Panther-dataset_v1](https://huggingface.co/datasets/Rardilit/Panther-dataset_v1)                                                                                                                                                                                                  | Panther                                             | Pairs                        | English                                              | 377 entries                                                                             | A dataset comes from the hh-rlhf. It rewrite hh-rlhf into the form of input-output pairs.                                                                                                              |
| [Baize Dataset](https://github.com/project-baize/baize-chatbot/tree/main/data)                                                                                                                                                                                                     | Baize                                               | Dialog                       | English                                              | 100K dialogs                                                                            | A dialog dataset generated by GPT-4 using self-talking. Questions and topics are collected from Quora, StackOverflow and some medical knowledge source.                                                |
| [h2ogpt-fortune2000<br/>personalized](https://huggingface.co/datasets/h2oai/h2ogpt-fortune2000-personalized)                                                                                                                                                                       | h2ogpt                                              | Pairs                        | English                                              | 11363 entries                                                                           | A instruction finetune developed by h2oai, covered various topics.                                                                                                                                     |
| [SHP](https://huggingface.co/datasets/stanfordnlp/SHP)                                                                                                                                                                                                                             | StableVicuna,<br/>chat-opt,<br/>, SteamSHP          | RLHF                         | English                                              | 385K entries                                                                            | An RLHF dataset different from previously mentioned ones, it use scores+timestamps to infer the users' preferences. Covers 18 domains, collected by Stanford.                                          |
| [ELI5](https://huggingface.co/datasets/eli5#source-data)                                                                                                                                                                                                                           | MiniLM series                                       | FT,<br/>RLHF                 | English                                              | 270K entries                                                                            | Questions and Answers collected from Reddit, including score. Might be used for RLHF reward model training.                                                                                            |
| [evol_instruct_70k](https://huggingface.co/datasets/victor123/evol_instruct_70k)                                                                                                                                                                                                   | WizardLM                                            | Pairs                        | English                                              |                                                                                         | An instruction finetune dataset derived from Alpaca-52K, using the **evolution** method in [this paper](https://arxiv.org/pdf/2304.12244.pdf)                                                          |
| [MOSS SFT data](https://github.com/OpenLMLab/MOSS/tree/main/SFT_data)                                                                                                                                                                                                              | MOSS                                                | Pairs,<br/>Dialog            | Chinese, English                                     | 1.1M entries                                                                            | A conversational dataset collected and developed by MOSS team. It has usefulness, loyalty and harmlessness labels for every data entries.                                                              |
| [ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)                                                                                                                                                                                                                 | Koala, Stable LLM                                   | Pairs                        | Multilingual                                         | 52K                                                                                     | This dataset comprises conversations collected from ShareGPT, with a specific focus on customized creative conversation.                                                                               |
| [GPT-4all Dataset](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations)                                                                                                                                                                                          | GPT-4all                                            | Pairs                        | English, <br/> Might have <br/> a translated version | 400k entries                                                                            | A combination of some subsets of OIG, P3 and Stackoverflow. Covers topics like general QA, customized creative questions.                                                                              |
| [COIG](https://huggingface.co/datasets/BAAI/COIG)                                                                                                                                                                                                                                  | /                                                   | Pairs                        | Chinese,<br/>code                                    | 200K entries                                                                            | A Chinese-based dataset. It contains domains like general purpose QA, Chinese exams, code. Its quality is checked by human annotators.                                                                 |
| [RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)                                                                                                                                                                                            | RedPajama                                           | PT                           | Primarily English                                    | 1.2T tokens <br/> 5TB                                                                   | A fully open pretraining dataset follows the LLaMA's method.                                                                                                                                           |
| [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1)                                                                                                                                                                                                                     | OpenAssistant                                       | Pairs,<br/> Dialog           | Multilingual<br/>(English, Spanish, etc.)            | 66,497 conversation trees                                                               | A large, human-written, human-annotated high quality conversation dataset. It aims at making LLM generates more natural response.                                                                      |
| [Alpaca-COT](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)                                                                                                                                                                                                                  | Phoenix                                             | Pairs,<br/> Dialog,<br/> CoT | English                                              | /                                                                                       | A mixture a many dataset like classic Alpaca dataset, OIG, Guanaco and some CoT(Chain-of-Thought) datasets like FLAN-CoT. May be handy to use.                                                         |
| [Bactrian-X](https://huggingface.co/datasets/MBZUAI/Bactrian-X)                                                                                                                                                                                                                    | /                                                   | Pairs                        | Multilingual<br/> (52 languages)                     | 67K entries per language                                                                | A multilingual version of **Alpaca** and **Dolly-15K**.                                                                                                                                                |
| [databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data) <br/> [zh-cn Ver](https://huggingface.co/datasets/jaja7744/dolly-15k-cn)                                                                                                                          | Dolly2.0                                            | Pairs                        | English                                              | 15K+ entries                                                                            | A dataset of **human-written** prompts and responses, featuring tasks such as open-domain question-answering, brainstorming, summarization, and more.                                                  |
| [AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned)                                                                                                                                                                                                                 | Some Alpaca/ LLaMA-like models                      | Pairs                        | English                                              | /                                                                                       | Cleaned version of Alpaca, GPT_LLM and GPTeacher.                                                                                                                                                      |
| [GPT-4-LLM Dataset](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                                                                                                                                                                                                    | Some Alpaca-like models                             | Pairs,<br/> RLHF             | English,<br/> Chinese                                | 52K entries for English and Chinese respectively <br/> 9K entries unnatural-instruction | NOT the dataset used by GPT-4!! It is generated by GPT-4 and some other LLM for better Pairs and RLHF. It includes instruction data as well as comparison data in RLHF style.                          |
| [GPTeacher](https://github.com/teknium1/GPTeacher)                                                                                                                                                                                                                                 | /                                                   | Pairs                        | English                                              | 20k entries                                                                             | A dataset contains targets generated by GPT-4 and includes many of the same seed tasks as the Alpaca dataset, with the addition of some new tasks such as roleplay.                                    |
| [HC3](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)                                                                                                                                                                                                              | Koala                                               | RLHF                         | English,<br/> Chinese                                | 24322 English <br/> 12853 Chinese                                                       | A multi-domain, human-vs-ChatGPT comparison dataset. Can be used for reward model training or ChatGPT detector training.                                                                               |
| [Alpaca data](https://github.com/tatsu-lab/stanford_alpaca#data-release) <br/> [Download](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)                                                                                                                 | Alpaca, ChatGLM-finetune-LoRA, Koala                | Dialog,<br/> Pairs           | English                                              | 52K entries<br/>21.4MB                                                                  | A dataset generated by text-davinci-003 to improve language models' ability to follow human instruction.                                                                                               |
| [OIG](https://huggingface.co/datasets/laion/OIG) <br/> [OIG-small-chip2](https://huggingface.co/datasets/0-hero/OIG-small-chip2)                                                                                                                                                   | Pythia-Chat-Base-7B, GPT-NeoXT-Chat-Base-20B, Koala | Dialog,<br/> Pairs           | English,<br/> code                                   | 44M entries                                                                             | A large conversational instruction dataset with medium and high quality subsets *(OIG-small-chip2)* for multi-task learning.                                                                           |
| [ChatAlpaca data](https://github.com/cascip/ChatAlpaca)                                                                                                                                                                                                                            | /                                                   | Dialog,<br/> Pairs           | English,<br/> Chinese version coming soon            | 10k entries<br/>39.5MB                                                                  | A dataset aims to help researchers develop models for instruction-following in multi-turn conversations.                                                                                               |
| [InstructionWild](https://github.com/XueFuzhao/InstructionWild)                                                                                                                                                                                                                    | ColossalChat                                        | Pairs                        | English, Chinese                                     | 10K enreues                                                                             | A Alpaca-style dataset, but with seed tasks comes from chatgpt screenshot.                                                                                                                             |
| [Firefly(ÊµÅËê§)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)                                                                                                                                                                                                         | Firefly(ÊµÅËê§)                                         | Pairs                        | Chinese                                              | 1.1M entries<br/>1.17GB                                                                 | A Chinese instruction-tuning dataset with 1.1 million human-written examples across 23 tasks, but no conversation.                                                                                     |
| [BELLE](https://github.com/LianjiaTech/BELLE) <br/> [0.5M version](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) <br/> [1M version](https://huggingface.co/datasets/BelleGroup/train_1M_CN) <br/> [2M version](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | BELLE series, Chunhua (Êò•Âçé)                          | Pairs                        | Chinese                                              | 2.67B in total                                                                          | A Chinese instruction dataset similar to *Alpaca data* constructed by generating answers from seed tasks, but no conversation.                                                                         |
| [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset#guanacodataset)                                                                                                                                                                                     | Guanaco                                             | Dialog,<br/> Pairs           | English,<br/> Chinese,<br/> Japanese                 | 534,530 entries                                                                         | A multilingual instruction dataset for enhancing language models' capabilities in various linguistic tasks, such as natural language understanding and explicit content recognition.                   |
| [OpenAI WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)                                                                                                                                                                                                         | WebGPT's reward model, Koala                        | RLHF                         | English                                              | 19,578 pairs                                                                            | Data set used in WebGPT paper. Used for training reward model in RLHF.                                                                                                                                 |
| [OpenAI<br/>Summarization Comparison](https://huggingface.co/datasets/openai/summarize_from_feedback)                                                                                                                                                                              | Koala                                               | RLHF                         | English                                              | ~93K entries<br/>420MB                                                                  | A dataset of human feedback which helps training a reward model. The reward model was then used to train a summarization model to align with human preferences.                                        |
| [self-instruct](https://github.com/yizhongw/self-instruct)                                                                                                                                                                                                                         | /                                                   | Pairs                        | English                                              | 82K entries                                                                             | The dataset generated by using the well-known [self-instruction method](https://arxiv.org/abs/2212.10560)                                                                                              |
| [unnatural-instructions](https://github.com/orhonovich/unnatural-instructions)                                                                                                                                                                                                     | /                                                   | Pairs                        | English                                              | 240,670 examples                                                                        | An early attempt to use powerful model (text-davinci-002) to generate data.                                                                                                                            |
| [xP3 (and some variant)](https://huggingface.co/datasets/bigscience/xP3)                                                                                                                                                                                                           | BLOOMZ, mT0                                         | Pairs                        | Multilingual,<br/> code                              | 79M entries<br/>88GB                                                                    | An instruction dataset for improving language models' generalization ability, similar to *Natural Instruct*.                                                                                           |
| [Flan V2](https://github.com/google-research/FLAN/tree/main/flan/v2)                                                                                                                                                                                                               | /                                                   | /                            | English                                              | /                                                                                       | A dataset compiles datasets from Flan 2021, P3, Super-Natural Instructions, along with dozens more datasets into one and formats them into a mix of zero-shot, few-shot and chain-of-thought templates |
| [Natural Instruction](https://instructions.apps.allenai.org/) <br/> [GitHub&Download](https://github.com/allenai/natural-instructions)                                                                                                                                             | tk-instruct series                                  | Pairs, <br/> evaluation      | Multilingual                                         | /                                                                                       | A benchmark with over 1,600 tasks with instruction and definition for evaluating and improving language models' multi-task generalization under natural language instruction.                          |
| [CrossWOZ](https://github.com/thu-coai/CrossWOZ)                                                                                                                                                                                                                                   | /                                                   | Dialog                       | English,<br/>Chinese                                 | 6K dialogs                                                                              | The dataset introduced by [this paper](https://arxiv.org/pdf/2002.11893.pdf), mainly about tourism topic in Beijing, answers are generated automatically by rules.                                     |


#### Potential Overlaps ‚ö†Ô∏è

We consider row items as subject.

|                   | OIG     | hh-rlhf  | xP3     | natural instruct | AlpacaDataCleaned | GPT-4-LLM | Alpaca-CoT |
|-------------------|---------|----------|---------|------------------|-------------------|-----------|------------|
| OIG               | /       | contains | overlap | overlap          | overlap           |           | overlap    |
| hh-rlhf           | part of | /        |         |                  |                   |           | overlap    |
| xP3               | overlap |          | /       | overlap          |                   |           | overlap    |
| natural instruct  | overlap |          | overlap | /                |                   |           | overlap    |
| AlpacaDataCleaned | overlap |          |         |                  | /                 | overlap   | overlap    |
| GPT-4-LLM         |         |          |         |                  | overlap           | /         | overlap    |
| Alpaca-CoT        | overlap | overlap  | overlap | overlap          | overlap           | overlap   | /          |

## Open Datasets for Pretraining üü¢ :atom:
| Dataset name                                                                                                                                  | Used by                                                                        | Type                                 | Language                | Size        | Description Ô∏è                                                                                                                                     |
|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------|-------------------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------|
| [falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)                                                                 | tiiuae/falcon series                                                           | PT                                   | English                 | /           | A refined subset of CommonCrawl.                                                                                                                  |
| [CBook-150K](https://github.com/FudanNLPLAB/CBook-150K)                                                                                       | /                                                                              | PT, <br/> building dataset           | Chinese                 | 150K+ books | A raw Chinese books dataset. Need some preprocess pipeline.                                                                                       |
| [Common Crawl](https://commoncrawl.org/)                                                                                                      | LLaMA (After some process)                                                     | building datasets, <br/> PT          | /                       | /           | The most well-known raw dataset, rarely be used directly. One possible preprocess pipeline is [CCNet](https://github.com/facebookresearch/cc_net) |
| [nlp_Chinese_Corpus](https://github.com/brightmart/nlp_chinese_corpus)                                                                        | /                                                                              | PT,<br/>TF                           | Chinese                 | /           | A Chinese pretrain corpus. Includes Wikipedia, Baidu Baike, Baidu QA, some forums QA and news corpus.                                             |
| [The Pile (V1)](https://pile.eleuther.ai/)                                                                                                    | GLM (partly), LLaMA (partly), GPT-J, GPT-NeoX-20B, Cerebras-GPT 6.7B, OPT-175b | PT                                   | Multilingual,<br/> code | 825GB       | A diverse open-source language modeling dataset consisting of 22 smaller, high-quality datasets that includes many domains and tasks.             |
| C4 <br/> [Huggingface dataset](https://huggingface.co/datasets/c4) <br/> [TensorFlow dataset](https://www.tensorflow.org/datasets/catalog/c4) | Google T5 Series, LLaMA                                                        | PT                                   | English                 | 305GB       | A colossal, cleaned version of Common Crawl's web crawl corpus. Frequently be used.                                                               |
| [ROOTS](https://huggingface.co/bigscience-data)                                                                                               | BLOOM                                                                          | PT                                   | Multilingual,<br/> code | 1.6TB       | A diverse open-source dataset consisting of sub-datasets like Wikipedia and StackExchange for language modeling.                                  |
| [PushshPairs reddit](https://files.pushshPairs.io/reddit/) <br/> [paper](https://arxiv.org/pdf/2001.08435.pdf)                                | OPT-175b                                                                       | PT                                   | /                       | /           | Raw reddit data, one possible processing pipeline in [this paper](https://aclanthology.org/2021.eacl-main.24.pdf)                                 |
| [Gutenberg project](https://www.gutenberg.org/policy/robot_access.html)                                                                       | LLaMA                                                                          | PT                                   | Multilingual            | /           | A book dataset, mostly novels. Not be preprocessed.                                                                                               |
| [CLUECorpus](https://github.com/CLUEbenchmark/CLUE)                                                                                           | /                                                                              | PT, <br/> finetune, <br/> evaluation | Chinese                 | 100GB       | A Chinese pretraining Corpus sourced from *Common Crawl*.                                                                                         |


## Domain-specific Datasets üü¢ üí°

| Dataset name                                                                                                    | Used by | Type             | Language              | Size            | Description Ô∏è                                                                                                                                                         |
|-----------------------------------------------------------------------------------------------------------------|---------|------------------|-----------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ChatGPT-Jailbreak-Prompts](https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts) <br/> ‚ö†Ô∏èRISKY   | /       | /                | English               | 163KB file size | Prompts for bypassing the safety regulation of ChatGPT. Can be use for probing the harmlessness of LLMs                                                               |
| [awesome chinese<br/>legal resources](https://github.com/pengxiao-song/awesome-chinese-legal-resources)         | LaWGPT  | /                | Chinese               | /               | A collection of Chinese legal data for LLM training.                                                                                                                  |
| [Long Form](https://github.com/akoksal/LongForm)                                                                | /       | Pairs            | English               | 23.7K entries   | A dataset aims at improving the long text generation ability of LLM.                                                                                                  |
| [symbolic-instruction-tuning](https://huggingface.co/datasets/sail/symbolic-instruction-tuning)                 | /       | Pairs            | English,<br/> code    | 796             | A dataset focuses on the 'symbolic' tasks: like SQL coding, mathematical computation, etc.                                                                            |
| [Safety Prompt](https://github.com/thu-coai/Safety-Prompts)                                                     | /       | Evaluation  only | Chinese               | 100k entries    | Chinese safety prompts for evaluating and improving the safety of LLMs.                                                                                               |
| [Tapir-Cleaned](https://huggingface.co/datasets/MattiaL/tapir-cleaned-116k)                                     | /       | Pairs            | English,              | 116k entries    | This is a revised version of the DAISLab dataset of PairsTT rules, which has been thoroughly cleaned, scored, and adjusted for the purpose of instruction-tuning      |
| [instructional_<br/>codesearchnet_python](https://huggingface.co/datasets/Nan-Do/instructional_codesearchnet_python) | /       | Pairs            | English &<br/> Python | 192MB           | This dataset is a template generated instructional Python datastet generated from an annotated version of the code-search-net dataset for the Open-Assistant project. |
| [finance-alpaca](https://huggingface.co/datasets/gbharti/finance-alpaca)                                        | /       | Pairs            | English               | 1.3K entries    | An Alpaca-style dataset but focus on financial topics                                                                                                                 |

## Private Datasets üî¥
| Dataset name          | Used by            | Type | Language                              | Size  | Description Ô∏è                                                                                   |
|-----------------------|--------------------|------|---------------------------------------|-------|-------------------------------------------------------------------------------------------------|
| WebText(Reddit links) | GPT-2              | PT   | English                               | /     | Data crawled from Reddit and filtered for GPT-2 pretraining.                                    |
| MassiveText           | Gopher, Chinchilla | PT   | 99% English, 1% other(including code) |       |                                                                                                 |
| WuDao(ÊÇüÈÅì) Corpora     | GLM                | PT   | Chinese                               | 200GB | A large scale Chinese corpus, Possible component originally open-sourced but not available now. |
